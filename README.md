ğŸ§  SHL Assessment Recommendation Engine (GenAI + RAG)
=====================================================

A **web-based GenAI-powered Assessment Recommendation System** that recommends the most relevant SHL assessments for a given Job Description (JD), using **semantic retrieval, ranking logic, and LLM-based explanations**.

This project is built as part of the **SHL AI Intern â€“ Generative AI Assignment**.

ğŸš€ Key Features
---------------

*   ğŸ” **Semantic Retrieval (RAG)** using FAISS + sentence embeddings
    
*   ğŸ§  **GenAI-based Explanation** for why assessments are recommended
    
*   âš–ï¸ **Custom Re-ranking Logic** (skills, duration, keywords)
    
*   ğŸ“Š **Automated CSV Submission Format** (as required by SHL)
    
*   ğŸŒ **Deployed Web App** (Streamlit)
    
*   ğŸ”Œ **API-ready architecture** (FastAPI compatible)
    

ğŸ—ï¸ System Architecture
-----------------------
```
Job Description (Input)
        â†“
Query Enrichment & Cleaning
        â†“
Embedding Generation
        â†“
FAISS Vector Search (Retrieval)
        â†“
Rule-based Re-ranking
        â†“
Top-K Assessment Recommendations
        â†“
GenAI Explanation (LLM)
        â†“
CSV Output & Web UI
```
ğŸ§ª Data Sources
---------------

*   **SHL Product Catalog** (crawled from official SHL website)
    
*   **Provided Train & Test Dataset** (Excel â†’ CSV)
    

ğŸ§  Retrieval-Augmented Generation (RAG)
---------------------------------------

### ğŸ”¹ Retrieval

*   Sentence embeddings generated using **Sentence Transformers**
    
*   FAISS index built on assessment descriptions
    
*   Top-K relevant assessments retrieved per query
    

### ğŸ”¹ Re-ranking

*   Keyword matching (skills, roles)
    
*   Duration-based filtering
    
*   Score boosting for domain alignment
    

### ğŸ”¹ Generation (GenAI)

*   LLM generates **natural-language explanations**
    
*   Strict prompt design to:
    
    *   Avoid hallucinations
        
    *   Explain only why selected assessments fit the JD
        
    *   Not suggest unavailable assessments
        

ğŸ“„ CSV Submission Format (CRITICAL)
-----------------------------------

The system automatically generates output in **exact SHL-required format**:
```
Query,Assessment_URL
Query 1,Recommendation 1 (URL)
Query 1,Recommendation 2 (URL)
Query 1,Recommendation 3 (URL)
.
.
.
Query 2,Recommendation 1 (URL)
```
Users can **download the CSV directly** from the web interface.

ğŸŒ Deployment
-------------

*   **Frontend**: Streamlit Cloud (Free Tier)
    
*   **Backend Logic**: Modular Python (API-ready)
    
*   **LLM**: Free-tier LLM (Groq / Ollama fallback)
    

> âš ï¸ Note: On Streamlit free tier, the app may take a few seconds to wake up after inactivity.

ğŸ–¥ï¸ How to Run Locally
----------------------

### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/Abhilove-Goyal/SHL-Assessment-Recommendation-Engine.git  cd SHL-Assessment-Recommendation-Engine
```
### 2ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt 
```
### 3ï¸âƒ£ Set environment variables
```
GROQ_API_KEY=your_api_key  API_BASE_URL=http://127.0.0.1:8000  
```
### 4ï¸âƒ£ Run Streamlit app
```
streamlit run frontend/app.py  
```
ğŸ“ Project Structure
--------------------
```
shl-assessment-recommendation-engine/
â”œâ”€â”€ api/                  # FastAPI backend
â”œâ”€â”€ crawler/              # SHL catalog crawler
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original Excel files
â”‚   â””â”€â”€ processed/        # Cleaned CSVs + FAISS index
â”œâ”€â”€ embeddings/           # Vector index creation
â”œâ”€â”€ recommender/
â”‚   â”œâ”€â”€ retrieval.py     # Vector search (RAG retrieval)
â”‚   â”œâ”€â”€ rerank.py        # Rule-based re-ranking
â”‚   â”œâ”€â”€ rag_explainer.py # GenAI explanation module
â”‚   â””â”€â”€ duration_filter.py
â”œâ”€â”€ evaluation/           # Recall@K evaluation
â”œâ”€â”€ frontend/             # Streamlit web app
â”œâ”€â”€ outputs/              # Submission CSV
â”œâ”€â”€ approach.md           # Detailed methodology
â””â”€â”€ README.md
```
ğŸ“ˆ Evaluation
-------------

*   **Metric Used**: Recall@10
    
*   **Mean Recall@10**: ~0.50
    
*   Shows effective retrieval despite small catalog size
    

ğŸ§© Design Decisions
-------------------

*   Chose **RAG** over pure LLM to ensure:
    
    *   Deterministic recommendations
        
    *   No hallucinated assessments
        
*   Used **rule-based re-ranking** for explainability
    
*   Kept system **API-compatible** for scalability
    

ğŸ”® Future Improvements
----------------------

*   Add SHAP-style explainability for ranking
    
*   Expand catalog coverage
    
*   Replace rule-based re-ranking with learning-to-rank
    
*   Add authentication & logging
    

ğŸ“¬ Submission Notes
-------------------

*   âœ… Web URL provided
    
*   âœ… CSV format strictly followed
    
*   âœ… GenAI used responsibly
    
*   âœ… Fully reproducible pipeline
    

ğŸ‘¤ Author
---------

**Abhilove Goyal**ğŸ“§ [abhilovegoyal17@gmail.com](mailto:abhilovegoyal17@gmail.com)ğŸ”— [LinkedIn](https://linkedin.com/in/abhilove-goyal)ğŸ’» [GitHub](https://github.com/Abhilove-Goyal)
